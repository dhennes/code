{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginner's Guide to Reinforcement Learning\n",
    "\n",
    "## OpenAI Gym\n",
    "\n",
    "We will use the OpenAI Gym toolkit to explore some of the aspects of *reinforcement learning* and program our first *learning agent*. Gym is a toolkit for developing and comparing reinforcement learning algorithms. You can learn more about it here: https://gym.openai.com/docs/\n",
    "\n",
    "## Prerequisits\n",
    "- some basic knowledge of Python\n",
    "- a Python 2.7 or Python 3 installation (e.g., via Anaconda: https://www.anaconda.com/download/)\n",
    "- Jupyter Notebook (comes with Anaconda)\n",
    "- `pip` (to install dependencies)\n",
    "\n",
    "## Installation\n",
    "\n",
    "- you need to install **OpenAI Gym** (install via PyPI recommended)  \n",
    "    https://gym.openai.com/docs/#installation    \n",
    "\n",
    "```sh\n",
    "pip install gym\n",
    "```\n",
    "\n",
    "## The Frozen Lake environment\n",
    "\n",
    "![](frozenlake.png) <div style=\"text-align: center\"> photo credit: Shea Gunther </div>\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "If you step into one of those holes, you'll fall into the freezing water.\n",
    "At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "you navigate across the lake and retrieve the disc.\n",
    "~~However, the ice is slippery, so you won't always move in the direction you intend.~~\n",
    "The surface is described using a grid like the following\n",
    "\n",
    "```\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "```\n",
    "\n",
    "```\n",
    "S : starting point, safe\n",
    "F : frozen surface, safe\n",
    "H : hole, fall to your doom\n",
    "G : goal, where the frisbee is located\n",
    "```\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OpenAI gym and create the environment\n",
    "import gym\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "env = FrozenLakeEnv(is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment and get the initial state \n",
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the current state of the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the action space look like, i.e., what actions can we take?\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 \"Getting started\"\n",
    "- First, we want to get our agent safely across the frozen lake to recover the frisbee.\n",
    "- Call `env.step` multiple times to get the agent to the goal location in the lower, right corner (indicated by `G`).\n",
    "\n",
    "- The environment’s step function takes an `action` as input and returns four values. These are:\n",
    "    1. `state` (**object**): an environment-specific object representing the next state of the environment.\n",
    "    2. `reward` (**float**): amount of reward achieved by the previous action. Goal is always to increase your total reward.\n",
    "    3. `done` (**boolean**): tells us if the episode terminated and whether it’s time to reset the environment\n",
    "    4. `info` (**dict**): diagnostic information useful for debugging (not important in our case)\n",
    "    \n",
    "Example:\n",
    "```python\n",
    "state, reward, done, info = env.step(action)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# perform action here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 \"Random agent\"\n",
    "- Implement a **random agent** that interacts with then environment by taking random actions.\n",
    "- Repeat for 10 episodes and record if the agent reached the goal location.\n",
    "- Remember to `reset` the environment when the agent reached a *terminal* state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here goes your code ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 \"Choosing actions\"\n",
    "- Implement $\\epsilon$-greedy.\n",
    "- Recall:\n",
    "    - with probability $1-\\epsilon$ take the *greedy* action\n",
    "    - with probability $\\epsilon$ take a random action\n",
    "- Note: a random action might also be greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: you can generate a uniform random number between 0 and 1 by:\n",
    "import numpy as np\n",
    "u = np.random.rand()\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume the following Q-values are given for an arbritary state by a list\n",
    "Q = [0.2, 1, 3, 0]\n",
    "\n",
    "def egreedy(Q, epsilon=0.2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the cell below.\n",
    "- Do you observe a problem?\n",
    "- Can you explain what is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume all Q values are equal\n",
    "Q = [0, 0, 0, 0]\n",
    "for i in range(10):\n",
    "    print(egreedy(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 \"Q-Learning\"\n",
    "Now, let us implement Q-learning.\n",
    "\n",
    "Initialize Q values for all possible state-action pairs $s, a$, e.g., $\\forall s,a: Q(s,a)=0$\n",
    "1. Choose action $A$ to take in current state $S$ with $\\epsilon$-greedy\n",
    "2. Take action $A$ and observe reward $R$ and next state $S'$\n",
    "3. Update Q value for $S$, $A$:\n",
    "    $$ Q(S,A) \\gets Q(S,A) + \\alpha \\Big[ R + \\gamma \\max_{a} Q(S', a) - Q(S,A) \\Big]$$\n",
    "4. If $S'$ is not *terminal*, repeat from step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "alpha = 0.1 # the learning rate\n",
    "gamma = 0.9 # the discount factor\n",
    "num_episodes = 1000 # number of episodes\n",
    "\n",
    "# initialize Q values here\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    s, done = env.reset(), False\n",
    "    while not done:\n",
    "        \n",
    "        done = True # REMOVE THIS LINE\n",
    "\n",
    "        # choose action\n",
    "\n",
    "        # take action\n",
    "\n",
    "        # update Q values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 \"Exploit learned knowledge\"\n",
    "- Now use the learned knowledge to bring the agent across the lake!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
